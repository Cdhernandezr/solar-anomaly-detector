# Importaciones necesarias para este bloque
import mlflow
import mlflow.pytorch
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

# El decorador @transformer indica que este bloque transforma datos
@transformer
def evaluate_and_register_model(trainer, *args, **kwargs):
    """
    EvalÃºa el modelo entrenado en el conjunto de prueba y, si es suficientemente bueno,
    lo registra en el Model Registry de MLflow.
    
    Args:
        trainer: El objeto ModelTrainer que contiene el modelo entrenado del bloque anterior.
    """
    print("ğŸ§ª Iniciando evaluaciÃ³n del modelo final...")
    
    # --- 1. Evaluar el modelo ---
    # La funciÃ³n evaluate_on_test devuelve las mÃ©tricas y la figura de la matriz de confusiÃ³n
    final_metrics, confusion_matrix_fig = trainer.evaluate_on_test()
    
    # --- 2. Registrar mÃ©tricas y artefactos finales en el "Run" activo de MLflow ---
    # MLflow sigue registrando en el mismo "Run" iniciado en el bloque anterior
    
    print("ğŸ“Š Registrando mÃ©tricas y artefactos finales en MLflow...")
    mlflow.log_metrics({
        "final_accuracy": final_metrics['accuracy'],
        "final_precision": final_metrics['precision'],
        "final_recall": final_metrics['recall'],
        "final_f1_score": final_metrics['f1_score']
    })
    
    # Guarda la figura de la matriz de confusiÃ³n
    mlflow.log_figure(confusion_matrix_fig, "confusion_matrix.png")
    
    # Guarda el modelo usando la integraciÃ³n de MLflow con PyTorch
    # Esto es crucial para poder registrarlo despuÃ©s
    mlflow.pytorch.log_model(trainer.model, "model")

    print("âœ… Modelo, mÃ©tricas y artefactos registrados en el Run de MLflow.")

    # --- 3. LÃ³gica de MLOps: El Control de Calidad y Registro del Modelo ---
    
    ACCURACY_THRESHOLD = 0.88  # Definimos nuestro umbral de calidad
    
    print(f"âš–ï¸ Verificando si la precisiÃ³n ({final_metrics['accuracy']:.4f}) supera el umbral ({ACCURACY_THRESHOLD})...")
    
    if final_metrics['accuracy'] >= ACCURACY_THRESHOLD:
        print(f"ğŸ‘ Â¡El modelo superÃ³ el umbral! Registrando en el Model Registry...")
        
        # Obtenemos el ID del Run actual
        run_id = mlflow.active_run().info.run_id
        
        # La URI (direcciÃ³n) del modelo que acabamos de guardar como artefacto
        model_uri = f"runs:/{run_id}/model"
        
        # Registramos el modelo dÃ¡ndole un nombre Ãºnico en el registro
        mlflow.register_model(
            model_uri=model_uri,
            name="solar_anomaly_detector"
        )
        
        print("ğŸ‰ Â¡Modelo registrado exitosamente!")
    else:
        print("ğŸ‘ El modelo no superÃ³ el umbral de calidad. No se registrarÃ¡.")
        
    return {"accuracy": final_metrics['accuracy'], "threshold": ACCURACY_THRESHOLD}